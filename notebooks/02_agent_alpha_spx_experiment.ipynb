{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64343936",
   "metadata": {},
   "source": [
    "# agent-alpha SPX Experiment (All Prices + Universe-Filtered Metrics)\n",
    "\n",
    "This notebook runs the hypothesis -> blueprint -> AST pipeline using your real SPX inputs:\n",
    "\n",
    "- `prices` file: OHLCV for all tickers that have ever existed in the universe history\n",
    "- `universe` file: constituent membership snapshots (daily or month-end)\n",
    "\n",
    "Evaluation logic:\n",
    "\n",
    "1. Compute feature components and factor scores on **all** tickers in the price panel.\n",
    "2. Compute RankIC / ICIR / Ex-ante IR only on rows included by the universe mask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_root = cwd\n",
    "if not (repo_root / \"agent-alpha\").exists():\n",
    "    if (cwd.parent / \"agent-alpha\").exists():\n",
    "        repo_root = cwd.parent\n",
    "    elif (cwd.parent.parent / \"agent-alpha\").exists():\n",
    "        repo_root = cwd.parent.parent\n",
    "\n",
    "agent_root = repo_root / \"agent-alpha\"\n",
    "if str(agent_root) not in sys.path:\n",
    "    sys.path.insert(0, str(agent_root))\n",
    "\n",
    "print(\"agent_root:\", agent_root)\n",
    "print(\"data_dir:\", agent_root / \"agent_alpha\" / \"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f6462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from agent_alpha.workflow import AgentAlphaWorkflow\n",
    "\n",
    "DATA_DIR = agent_root / \"agent_alpha\" / \"data\"\n",
    "ALLOWED_DATA_EXTS = {\".csv\", \".txt\", \".parquet\"}\n",
    "\n",
    "\n",
    "def _read_table(path: Path) -> pd.DataFrame:\n",
    "    suffix = path.suffix.lower()\n",
    "    if suffix in {\".csv\", \".txt\"}:\n",
    "        return pd.read_csv(path)\n",
    "    if suffix == \".parquet\":\n",
    "        return pd.read_parquet(path)\n",
    "    raise ValueError(f\"Unsupported file extension: {path.suffix}\")\n",
    "\n",
    "\n",
    "def _normalize_name(name: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", str(name).strip().lower())\n",
    "\n",
    "\n",
    "def _pick_column(df: pd.DataFrame, aliases: Iterable[str], required: bool = True) -> str | None:\n",
    "    lookup = {_normalize_name(col): col for col in df.columns}\n",
    "    for alias in aliases:\n",
    "        key = _normalize_name(alias)\n",
    "        if key in lookup:\n",
    "            return lookup[key]\n",
    "    if required:\n",
    "        raise ValueError(f\"Missing required column. Tried aliases: {list(aliases)}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def _find_data_file(kind: str, data_dir: Path = DATA_DIR) -> Path:\n",
    "    preferred_names = {\n",
    "        \"universe\": [\n",
    "            \"spx_universe_filtered.csv\",\n",
    "            \"spx_universe.csv\",\n",
    "        ],\n",
    "        \"prices\": [\n",
    "            \"spx_prices.csv\",\n",
    "            \"spx_agent_panel.csv\",\n",
    "        ],\n",
    "    }\n",
    "    patterns = {\n",
    "        \"universe\": [\"*universe*\", \"*constituent*\", \"*membership*\", \"*spx*univ*\"],\n",
    "        \"prices\": [\"*prices*\", \"*price*\", \"*ohlcv*\", \"*panel*\", \"*bars*\", \"*spx*price*\"],\n",
    "    }\n",
    "    if kind not in patterns or kind not in preferred_names:\n",
    "        raise ValueError(f\"Unsupported kind: {kind}\")\n",
    "\n",
    "    for name in preferred_names[kind]:\n",
    "        path = data_dir / name\n",
    "        if path.is_file() and path.suffix.lower() in ALLOWED_DATA_EXTS:\n",
    "            return path\n",
    "\n",
    "    candidates: list[Path] = []\n",
    "    for pattern in patterns[kind]:\n",
    "        for path in data_dir.rglob(pattern):\n",
    "            if path.is_file() and path.suffix.lower() in ALLOWED_DATA_EXTS:\n",
    "                candidates.append(path)\n",
    "\n",
    "    unique = sorted(set(candidates))\n",
    "    if not unique:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No {kind} file found in {data_dir}. \"\n",
    "            f\"Expected one of extensions {sorted(ALLOWED_DATA_EXTS)} and a filename containing: {patterns[kind]}\"\n",
    "        )\n",
    "\n",
    "    if len(unique) > 1:\n",
    "        print(f\"[{kind}] multiple files found; using first candidate after sort:\")\n",
    "        for path in unique:\n",
    "            print(\" -\", path.relative_to(data_dir))\n",
    "\n",
    "    return unique[0]\n",
    "\n",
    "\n",
    "def _parse_membership(series: pd.Series) -> pd.Series:\n",
    "    if series.dtype == bool:\n",
    "        return series.astype(int)\n",
    "\n",
    "    numeric = pd.to_numeric(series, errors=\"coerce\")\n",
    "    if numeric.notna().any():\n",
    "        return (numeric.fillna(0) != 0).astype(int)\n",
    "\n",
    "    text = series.astype(str).str.strip().str.lower()\n",
    "    true_values = {\"1\", \"true\", \"t\", \"yes\", \"y\", \"in\", \"member\"}\n",
    "    return text.isin(true_values).astype(int)\n",
    "\n",
    "\n",
    "def standardize_universe(universe_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    date_col = _pick_column(universe_raw, [\"date\", \"datetime\", \"timestamp\", \"trading_date\"], required=False)\n",
    "    ticker_col = _pick_column(universe_raw, [\"ticker\", \"symbol\", \"instrument\", \"ric\", \"asset\"], required=True)\n",
    "    flag_col = _pick_column(\n",
    "        universe_raw,\n",
    "        [\"in_universe\", \"is_member\", \"in_index\", \"member\", \"active\", \"weight\"],\n",
    "        required=False,\n",
    "    )\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"ticker\"] = universe_raw[ticker_col].astype(str).str.upper().str.replace(\".\", \"-\", regex=False).str.strip()\n",
    "    out[\"ticker\"] = out[\"ticker\"].replace({\"\": np.nan})\n",
    "\n",
    "    if date_col is None:\n",
    "        out[\"date\"] = pd.NaT\n",
    "    else:\n",
    "        out[\"date\"] = pd.to_datetime(universe_raw[date_col], errors=\"coerce\").dt.tz_localize(None).dt.normalize()\n",
    "\n",
    "    if flag_col is None:\n",
    "        out[\"in_universe\"] = 1\n",
    "    else:\n",
    "        parsed = _parse_membership(universe_raw[flag_col])\n",
    "        if _normalize_name(flag_col) == \"weight\":\n",
    "            parsed = (pd.to_numeric(universe_raw[flag_col], errors=\"coerce\").fillna(0.0) > 0).astype(int)\n",
    "        out[\"in_universe\"] = parsed.astype(int)\n",
    "\n",
    "    out = out.dropna(subset=[\"ticker\"])\n",
    "    out = out[out[\"in_universe\"] == 1].copy()\n",
    "\n",
    "    if out[\"date\"].notna().any():\n",
    "        out = out.dropna(subset=[\"date\"])\n",
    "        out = out.drop_duplicates([\"date\", \"ticker\"])\n",
    "        out = out.sort_values([\"date\", \"ticker\"])\n",
    "    else:\n",
    "        out = out.drop_duplicates([\"ticker\"])\n",
    "        out = out.sort_values([\"ticker\"])\n",
    "\n",
    "    return out.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def standardize_prices(prices_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    date_col = _pick_column(prices_raw, [\"date\", \"datetime\", \"timestamp\", \"trading_date\"], required=True)\n",
    "    ticker_col = _pick_column(prices_raw, [\"ticker\", \"symbol\", \"instrument\", \"ric\", \"asset\"], required=True)\n",
    "    open_col = _pick_column(prices_raw, [\"open\", \"$open\", \"px_open\", \"o\"], required=True)\n",
    "    high_col = _pick_column(prices_raw, [\"high\", \"$high\", \"px_high\", \"h\"], required=True)\n",
    "    low_col = _pick_column(prices_raw, [\"low\", \"$low\", \"px_low\", \"l\"], required=True)\n",
    "    close_col = _pick_column(prices_raw, [\"close\", \"adj_close\", \"$close\", \"px_close\", \"c\"], required=True)\n",
    "    volume_col = _pick_column(prices_raw, [\"volume\", \"$volume\", \"vol\", \"v\"], required=True)\n",
    "\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"date\": pd.to_datetime(prices_raw[date_col], errors=\"coerce\").dt.tz_localize(None).dt.normalize(),\n",
    "            \"ticker\": prices_raw[ticker_col].astype(str).str.upper().str.replace(\".\", \"-\", regex=False).str.strip(),\n",
    "            \"open\": pd.to_numeric(prices_raw[open_col], errors=\"coerce\"),\n",
    "            \"high\": pd.to_numeric(prices_raw[high_col], errors=\"coerce\"),\n",
    "            \"low\": pd.to_numeric(prices_raw[low_col], errors=\"coerce\"),\n",
    "            \"close\": pd.to_numeric(prices_raw[close_col], errors=\"coerce\"),\n",
    "            \"volume\": pd.to_numeric(prices_raw[volume_col], errors=\"coerce\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    out = out.replace([np.inf, -np.inf], np.nan)\n",
    "    out = out.dropna(subset=[\"date\", \"ticker\", \"open\", \"high\", \"low\", \"close\"])\n",
    "    out[\"volume\"] = out[\"volume\"].fillna(0.0)\n",
    "    out = out[(out[\"high\"] >= out[\"low\"]) & (out[\"volume\"] >= 0)]\n",
    "    out = out.drop_duplicates([\"date\", \"ticker\"]).sort_values([\"date\", \"ticker\"])\n",
    "    return out.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def build_panel_for_agent(prices_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if prices_df.empty:\n",
    "        raise ValueError(\"Prices input is empty after normalization\")\n",
    "\n",
    "    panel = prices_df.rename(\n",
    "        columns={\n",
    "            \"date\": \"datetime\",\n",
    "            \"ticker\": \"instrument\",\n",
    "            \"open\": \"$open\",\n",
    "            \"high\": \"$high\",\n",
    "            \"low\": \"$low\",\n",
    "            \"close\": \"$close\",\n",
    "            \"volume\": \"$volume\",\n",
    "        }\n",
    "    )\n",
    "    panel = panel.set_index([\"datetime\", \"instrument\"]).sort_index()\n",
    "    panel.index = panel.index.set_names([\"datetime\", \"instrument\"])\n",
    "    panel = panel[[\"$open\", \"$high\", \"$low\", \"$close\", \"$volume\"]]\n",
    "    return panel\n",
    "\n",
    "\n",
    "def build_universe_mask_for_agent(universe_df: pd.DataFrame, panel: pd.DataFrame) -> pd.DataFrame:\n",
    "    if universe_df.empty:\n",
    "        raise ValueError(\"Universe input is empty after normalization\")\n",
    "\n",
    "    out = universe_df[[\"date\", \"ticker\", \"in_universe\"]].copy()\n",
    "    out[\"ticker\"] = out[\"ticker\"].astype(str).str.upper().str.replace(\".\", \"-\", regex=False).str.strip()\n",
    "\n",
    "    panel_tickers = set(panel.index.get_level_values(\"instrument\").astype(str).str.upper())\n",
    "    out = out[out[\"ticker\"].isin(panel_tickers)].copy()\n",
    "    if out.empty:\n",
    "        raise ValueError(\"No overlap between universe tickers and price panel tickers\")\n",
    "\n",
    "    if out[\"date\"].notna().any():\n",
    "        out = out.dropna(subset=[\"date\"]).drop_duplicates([\"date\", \"ticker\"]).sort_values([\"date\", \"ticker\"])\n",
    "    else:\n",
    "        out = out.drop_duplicates([\"ticker\"]).sort_values([\"ticker\"])\n",
    "\n",
    "    return out.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def panel_profile(panel: pd.DataFrame) -> dict[str, float | int | str]:\n",
    "    dates = panel.index.get_level_values(\"datetime\")\n",
    "    instruments = panel.index.get_level_values(\"instrument\")\n",
    "    close = panel[\"$close\"].astype(float)\n",
    "    rets = close.groupby(level=\"instrument\", sort=False).pct_change()\n",
    "    dollar_vol = (panel[\"$close\"] * panel[\"$volume\"]).groupby(level=\"datetime\").median()\n",
    "\n",
    "    return {\n",
    "        \"rows\": int(len(panel)),\n",
    "        \"n_dates\": int(dates.nunique()),\n",
    "        \"n_tickers\": int(instruments.nunique()),\n",
    "        \"start\": str(dates.min().date()),\n",
    "        \"end\": str(dates.max().date()),\n",
    "        \"median_abs_1d_return_pct\": float(rets.abs().median() * 100.0),\n",
    "        \"median_daily_dollar_volume\": float(dollar_vol.median()),\n",
    "    }\n",
    "\n",
    "\n",
    "def universe_profile(universe_mask: pd.DataFrame) -> dict[str, int | str]:\n",
    "    has_dates = bool(universe_mask[\"date\"].notna().any())\n",
    "    snapshot_dates = int(universe_mask[\"date\"].dropna().nunique()) if has_dates else 0\n",
    "    return {\n",
    "        \"rows\": int(len(universe_mask)),\n",
    "        \"n_tickers\": int(universe_mask[\"ticker\"].nunique()),\n",
    "        \"has_dates\": str(has_dates),\n",
    "        \"snapshot_dates\": snapshot_dates,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f00c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "universe_path = _find_data_file(\"universe\")\n",
    "prices_path = _find_data_file(\"prices\")\n",
    "\n",
    "print(\"universe_path:\", universe_path)\n",
    "print(\"prices_path:\", prices_path)\n",
    "\n",
    "universe_raw = _read_table(universe_path)\n",
    "prices_raw = _read_table(prices_path)\n",
    "\n",
    "universe_df = standardize_universe(universe_raw)\n",
    "prices_df = standardize_prices(prices_raw)\n",
    "panel = build_panel_for_agent(prices_df)\n",
    "universe_mask = build_universe_mask_for_agent(universe_df, panel)\n",
    "\n",
    "profile = panel_profile(panel)\n",
    "univ = universe_profile(universe_mask)\n",
    "print(\"panel_profile:\", profile)\n",
    "print(\"universe_profile:\", univ)\n",
    "\n",
    "display(universe_mask.head(3))\n",
    "display(prices_df.head(3))\n",
    "display(panel.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083084d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set. Export it before running LLM steps.\")\n",
    "\n",
    "MODEL_NAME = \"gpt-5-mini\"\n",
    "\n",
    "llm_data_context = (\n",
    "    f\"SPX panel from {profile['start']} to {profile['end']}; \"\n",
    "    f\"{profile['n_tickers']} names, {profile['n_dates']} trading days, {profile['rows']} rows. \"\n",
    "    f\"Median abs 1D return is {profile['median_abs_1d_return_pct']:.3f}%. \"\n",
    "    \"Focus on robust, interpretable cross-sectional OHLCV factors.\"\n",
    ")\n",
    "\n",
    "user_goal = (\n",
    "    \"Generate a robust SPX cross-sectional alpha hypothesis and express it as a compact factor blueprint. \"\n",
    "    + llm_data_context\n",
    ")\n",
    "\n",
    "print(user_goal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = AgentAlphaWorkflow(\n",
    "    model_name=MODEL_NAME,\n",
    "    periods=(1, 5, 10),\n",
    "    max_attempts=2,\n",
    ")\n",
    "\n",
    "state = workflow.run(\n",
    "    user_goal=user_goal,\n",
    "    panel=panel,\n",
    "    max_attempts=2,\n",
    "    universe_mask=universe_mask,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ad655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metrics = state.get(\"metrics\", {})\n",
    "\n",
    "print(\"error:\", state.get(\"error\"))\n",
    "print(\"\n",
    "hypothesis:\n",
    "\", state.get(\"hypothesis\"))\n",
    "print(\"\n",
    "rationale:\n",
    "\", state.get(\"rationale\"))\n",
    "print(\"\n",
    "AST expression:\n",
    "\", state.get(\"ast_expression\"))\n",
    "print(\"\n",
    "AST summary:\n",
    "\", state.get(\"ast_summary\"))\n",
    "print(\"\n",
    "evaluation_scope:\n",
    "\", json.dumps(metrics.get(\"evaluation_scope\", {}), indent=2, ensure_ascii=False))\n",
    "print(\"\n",
    "metrics:\n",
    "\", json.dumps(metrics, indent=2, ensure_ascii=False))\n",
    "print(\"\n",
    "blueprint:\n",
    "\", json.dumps(state.get(\"blueprint_json\", {}), indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c605dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = state.get(\"factor\")\n",
    "metrics = state.get(\"metrics\", {})\n",
    "scope = metrics.get(\"evaluation_scope\", {})\n",
    "\n",
    "if factor is None:\n",
    "    print(\"No factor returned. Check state['error'].\")\n",
    "else:\n",
    "    print(\"factor_non_null_all_tickers:\", int(factor.notna().sum()))\n",
    "    print(\"rows_in_evaluation_scope:\", scope.get(\"rows_in_scope\"))\n",
    "    print(\"n_tickers_in_evaluation_scope:\", scope.get(\"n_tickers_in_scope\"))\n",
    "    display(factor.dropna().head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
